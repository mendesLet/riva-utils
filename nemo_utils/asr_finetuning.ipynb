{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SzX9W3O1im-"
      },
      "source": [
        "### Imports and stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKMWAsLv1f-a"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install sox libsndfile1 ffmpeg libsox-fmt-mp3\n",
        "!pip install text-unidecode\n",
        "!pip install matplotlib>=3.3.2\n",
        "\n",
        "## Install NeMo\n",
        "BRANCH = 'main'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVPQWWH340qi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import tarfile\n",
        "import wget\n",
        "import copy\n",
        "from omegaconf import OmegaConf, open_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfxjhWi0414b"
      },
      "outputs": [],
      "source": [
        "data_dir = 'datasets/'\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "  os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(\"scripts\"):\n",
        "  os.makedirs(\"scripts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGxotlrL42tq"
      },
      "outputs": [],
      "source": [
        "import nemo\n",
        "import nemo.collections.asr as nemo_asr\n",
        "from nemo.collections.asr.metrics.wer import word_error_rate\n",
        "from nemo.utils import logging, exp_manager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50SVS-gm1lpT"
      },
      "source": [
        "### Dataset stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bp7s4652Zok"
      },
      "source": [
        "#### Getting dataset from hugging face and converting to nemo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrVJhbSp1v4s"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0mLn61M13Rw"
      },
      "outputs": [],
      "source": [
        "VERSION = \"lemorim/noisy-dataset\" # dataset name\n",
        "LANGUAGE = \"default\" # dataset language\n",
        "manifest_dir = os.path.join('datasets', LANGUAGE, VERSION, LANGUAGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ61_hnO1z3w"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"convert_hf_dataset_to_nemo.py\"):\n",
        "    !wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/speech_recognition/convert_hf_dataset_to_nemo.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN6t9tC71rI6"
      },
      "outputs": [],
      "source": [
        "!python convert_hf_dataset_to_nemo.py \\\n",
        "    output_dir=datasets/$LANGUAGE \\\n",
        "    path=$VERSION \\\n",
        "    name=$LANGUAGE \\\n",
        "    split=\"train\" \\\n",
        "    ensure_ascii=False \\\n",
        "    use_auth_token=True\n",
        "\n",
        "!python convert_hf_dataset_to_nemo.py \\\n",
        "    output_dir=datasets/$LANGUAGE \\\n",
        "    path=$VERSION \\\n",
        "    name=$LANGUAGE \\\n",
        "    split=\"validation\" \\\n",
        "    ensure_ascii=False \\\n",
        "    use_auth_token=True\n",
        "\n",
        "!python convert_hf_dataset_to_nemo.py \\\n",
        "    output_dir=datasets/$LANGUAGE \\\n",
        "    path=$VERSION \\\n",
        "    name=$LANGUAGE \\\n",
        "    split=\"test\" \\\n",
        "    ensure_ascii=False \\\n",
        "    use_auth_token=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7XIaFRN1-9Z"
      },
      "outputs": [],
      "source": [
        "train_manifest = f\"{manifest_dir}/train/train_lemorim_noisy-dataset_manifest.json\"\n",
        "dev_manifest = f\"{manifest_dir}/validation/validation_lemorim_noisy-dataset_manifest.json\"\n",
        "test_manifest = f\"{manifest_dir}/test/test_lemorim_noisy-dataset_manifest.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbr9KsMK2eLY"
      },
      "source": [
        "#### Preparing dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9niC2IaN2iZ9"
      },
      "outputs": [],
      "source": [
        "# Manifest Utils\n",
        "from tqdm.auto import tqdm\n",
        "from nemo.collections.asr.parts.utils.manifest_utils import read_manifest, write_manifest\n",
        "import json\n",
        "\n",
        "\n",
        "def write_processed_manifest(data, original_path):\n",
        "    original_manifest_name = os.path.basename(original_path)\n",
        "    new_manifest_name = original_manifest_name.replace(\".json\", \"_processed.json\")\n",
        "\n",
        "    manifest_dir = os.path.split(original_path)[0]\n",
        "    filepath = os.path.join(manifest_dir, new_manifest_name)\n",
        "    write_manifest(filepath, data)\n",
        "    print(f\"Finished writing manifest: {filepath}\")\n",
        "    return filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrQvmCCA2kcH"
      },
      "outputs": [],
      "source": [
        "train_manifest_data = read_manifest(train_manifest)\n",
        "dev_manifest_data = read_manifest(dev_manifest)\n",
        "test_manifest_data = read_manifest(test_manifest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DX6IOXz2lWf"
      },
      "outputs": [],
      "source": [
        "train_text = [data['text'] for data in train_manifest_data]\n",
        "dev_text = [data['text'] for data in dev_manifest_data]\n",
        "test_text = [data['text'] for data in test_manifest_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jd93Ug-24Lk"
      },
      "source": [
        "#### Removing special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRuzehf326fp"
      },
      "outputs": [],
      "source": [
        "# Preprocessing steps\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\…\\{\\}\\【\\】\\・\\。\\『\\』\\、\\ー\\〜]'  # remove special character tokens\n",
        "# kanji_removal_regex = '[' + \"\".join([f\"\\{token}\" for token in extra_kanji]) + ']'  # remove test set kanji\n",
        "\n",
        "\n",
        "def remove_special_characters(data):\n",
        "    data[\"text\"] = re.sub(chars_to_ignore_regex, '', data[\"text\"]).lower().strip()\n",
        "    return data\n",
        "\n",
        "# Processing pipeline\n",
        "def apply_preprocessors(manifest, preprocessors):\n",
        "    for processor in preprocessors:\n",
        "        for idx in tqdm(range(len(manifest)), desc=f\"Applying {processor.__name__}\"):\n",
        "            manifest[idx] = processor(manifest[idx])\n",
        "\n",
        "    print(\"Finished processing manifest !\")\n",
        "    return manifest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMNEjFL93BWM"
      },
      "outputs": [],
      "source": [
        "# List of pre-processing functions\n",
        "PREPROCESSORS = [\n",
        "    remove_special_characters,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vUsQCZN3Cnp"
      },
      "outputs": [],
      "source": [
        "# Load manifests\n",
        "train_data = read_manifest(train_manifest)\n",
        "dev_data = read_manifest(dev_manifest)\n",
        "test_data = read_manifest(test_manifest)\n",
        "\n",
        "# Apply preprocessing\n",
        "train_data_processed = apply_preprocessors(train_data, PREPROCESSORS)\n",
        "dev_data_processed = apply_preprocessors(dev_data, PREPROCESSORS)\n",
        "test_data_processed = apply_preprocessors(test_data, PREPROCESSORS)\n",
        "\n",
        "# Write new manifests\n",
        "train_manifest_cleaned = write_processed_manifest(train_data_processed, train_manifest)\n",
        "dev_manifest_cleaned = write_processed_manifest(dev_data_processed, dev_manifest)\n",
        "test_manifest_cleaned = write_processed_manifest(test_data_processed, test_manifest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Character set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_charset(manifest_data):\n",
        "    charset = defaultdict(int)\n",
        "    for row in tqdm(manifest_data, desc=\"Computing character set\"):\n",
        "        text = row['text']\n",
        "        for character in text:\n",
        "            charset[character] += 1\n",
        "    return charset\n",
        "\n",
        "train_charset = get_charset(train_manifest_data)\n",
        "dev_charset = get_charset(dev_manifest_data)\n",
        "test_charset = get_charset(test_manifest_data)\n",
        "\n",
        "train_dev_set = set.union(set(train_charset.keys()), set(dev_charset.keys()))\n",
        "test_set = set(test_charset.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWaHz3Y31qsV"
      },
      "source": [
        "### Actually finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVYb0WRy1veY"
      },
      "outputs": [],
      "source": [
        "# list all asr models available\n",
        "# nemo_asr.models.ASRModel.list_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIQG_IOO3UeG"
      },
      "source": [
        "#### Character encoding CTC Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4IWmPuI3Rjv"
      },
      "outputs": [],
      "source": [
        "char_model = nemo_asr.models.ASRModel.from_pretrained(\"stt_en_conformer_ctc_large\", map_location='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtXO6GiS3Sqn"
      },
      "outputs": [],
      "source": [
        "char_model.setup_training_data(train_data_config={\n",
        "    \"manifest_filepath\": [\n",
        "        train_manifest_cleaned\n",
        "    ],\n",
        "    \"sample_rate\": 16000,\n",
        "    \"batch_size\": 1,\n",
        "    \"shuffle\": True,\n",
        "    \"num_workers\": 4,\n",
        "    \"pin_memory\": True,\n",
        "    \"use_start_end_token\": False,\n",
        "    \"trim_silence\": False,\n",
        "    \"max_duration\": 20.0,\n",
        "    \"min_duration\": 0.1,\n",
        "    \"is_tarred\": False,\n",
        "    \"shuffle_n\": 2048,\n",
        "    \"bucketing_strategy\": \"synced_randomized\",\n",
        "    \"bucketing_batch_size\": [34, 30, 26, 22, 18, 16, 12, 8]\n",
        "}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqMstTJT3smI"
      },
      "source": [
        "#### Setting up data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_WTinDJ3vnx"
      },
      "outputs": [],
      "source": [
        "cfg = copy.deepcopy(char_model.cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzLSYjE_3yN4"
      },
      "outputs": [],
      "source": [
        "# Setup train, validation, test configs\n",
        "with open_dict(cfg):\n",
        "  # Train dataset  (Concatenate train manifest cleaned and dev manifest cleaned)\n",
        "  cfg.train_ds.manifest_filepath = f\"{train_manifest_cleaned},{dev_manifest_cleaned}\"\n",
        "  cfg.train_ds.labels = list(train_dev_set)\n",
        "  cfg.train_ds.normalize_transcripts = False\n",
        "  cfg.train_ds.batch_size = 16\n",
        "  cfg.train_ds.num_workers = 8\n",
        "  cfg.train_ds.pin_memory = True\n",
        "  cfg.train_ds.trim_silence = True\n",
        "\n",
        "  # Validation dataset  (Use test dataset as validation, since we train using train + dev)\n",
        "  cfg.validation_ds.manifest_filepath = test_manifest_cleaned\n",
        "  cfg.validation_ds.labels = list(train_dev_set)\n",
        "  cfg.validation_ds.normalize_transcripts = False\n",
        "  cfg.validation_ds.batch_size = 8\n",
        "  cfg.validation_ds.num_workers = 8\n",
        "  cfg.validation_ds.pin_memory = True\n",
        "  cfg.validation_ds.trim_silence = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8uTJSNB3z8l"
      },
      "outputs": [],
      "source": [
        "# setup data loaders with new configs\n",
        "char_model.setup_training_data(cfg.train_ds)\n",
        "char_model.setup_multiple_validation_data(cfg.validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jbTrddZ31vr"
      },
      "source": [
        "#### Setting up optimizer and sceduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_UL6YJr37yL"
      },
      "outputs": [],
      "source": [
        "# Original optimizer + scheduler\n",
        "print(OmegaConf.to_yaml(char_model.cfg.optim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1JH0aeB3-Yu"
      },
      "outputs": [],
      "source": [
        "with open_dict(char_model.cfg.optim):\n",
        "  char_model.cfg.optim.lr = 0.01\n",
        "  char_model.cfg.optim.betas = [0.95, 0.5]  # from paper\n",
        "  char_model.cfg.optim.weight_decay = 0.001  # Original weight decay\n",
        "  char_model.cfg.optim.sched.warmup_steps = None  # Remove default number of steps of warmup\n",
        "  char_model.cfg.optim.sched.warmup_ratio = 0.05  # 5 % warmup\n",
        "  char_model.cfg.optim.sched.min_lr = 1e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrGsQXB44CTm"
      },
      "source": [
        "#### Setting up augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjw6cb9H4EYG"
      },
      "outputs": [],
      "source": [
        "print(OmegaConf.to_yaml(char_model.cfg.spec_augment))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNIK14eQ4FZN"
      },
      "outputs": [],
      "source": [
        "char_model.spec_augmentation = char_model.from_config_dict(char_model.cfg.spec_augment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JObDLhPG4JQf"
      },
      "source": [
        "#### Setup metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ht03BoR4LSx"
      },
      "outputs": [],
      "source": [
        "use_cer = True\n",
        "log_prediction = True\n",
        "\n",
        "char_model.wer.use_cer = use_cer\n",
        "char_model.wer.log_prediction = log_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSRC5BMW4SGh"
      },
      "source": [
        "#### Setup Trainer and experiment manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H45VqE7B4RlW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pytorch_lightning as ptl\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  accelerator = 'gpu'\n",
        "else:\n",
        "  accelerator = 'cpu'\n",
        "\n",
        "EPOCHS = 30  # 100 epochs would provide better results, but would take an hour to train\n",
        "\n",
        "trainer = ptl.Trainer(devices=1,\n",
        "                      accelerator=accelerator,\n",
        "                      max_epochs=EPOCHS,\n",
        "                      accumulate_grad_batches=1,\n",
        "                      enable_checkpointing=False,\n",
        "                      logger=False,\n",
        "                      log_every_n_steps=5,\n",
        "                      check_val_every_n_epoch=10)\n",
        "\n",
        "# Setup model with the trainer\n",
        "char_model.set_trainer(trainer)\n",
        "\n",
        "# Finally, update the model's internal config\n",
        "char_model.cfg = char_model._cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3jhDBYX4Ydb"
      },
      "outputs": [],
      "source": [
        "# Environment variable generally used for multi-node multi-gpu training.\n",
        "# In notebook environments, this flag is unnecessary and can cause logs of multiple training runs to overwrite each other.\n",
        "os.environ.pop('NEMO_EXPM_VERSION', None)\n",
        "\n",
        "config = exp_manager.ExpManagerConfig(\n",
        "    exp_dir=f'experiments/lang-{LANGUAGE}/',\n",
        "    name=f\"ASR-Char-Model-Language-{LANGUAGE}\",\n",
        "    checkpoint_callback_params=exp_manager.CallbackParams(\n",
        "        monitor=\"val_wer\",\n",
        "        mode=\"min\",\n",
        "        always_save_nemo=True,\n",
        "        save_best_model=True,\n",
        "    ),\n",
        ")\n",
        "\n",
        "config = OmegaConf.structured(config)\n",
        "\n",
        "logdir = exp_manager.exp_manager(trainer, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDwCFQDP4Z87"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google import colab\n",
        "  COLAB_ENV = True\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "  COLAB_ENV = False\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "if COLAB_ENV:\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir /content/experiments/lang-$LANGUAGE/ASR-Char-Model-Language-$LANGUAGE/\n",
        "else:\n",
        "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhjgaq5j4cXA"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "trainer.fit(char_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QlIVTZu4f1A"
      },
      "source": [
        "### Save final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJacfpVV4iRK"
      },
      "outputs": [],
      "source": [
        "save_path = f\"{VERSION.split(\"/\")[-1]}_{LANGUAGE}.nemo\"\n",
        "char_model.save_to(f\"{save_path}\")\n",
        "print(f\"Model saved at path : {os.getcwd() + os.path.sep + save_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
